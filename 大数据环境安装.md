# 大数据环境安装

## 安装虚拟机

安装centos7虚拟机

设置固定IP地址

进入文件

~~~shell
cd /etc/sysconfig/network-scripts/
~~~

安装vim

~~~shell
yum install -y vim
~~~

打开对应的网卡文件

~~~shell
vim / vi ifcfg-ens33
~~~

~~~shell
TYPE="Ethernet"
BOOTPROTO="static"#修改这里
DEFROUTE="yes"
PEERDNS="yes"
PEERROUTES="yes"
IPV4_FAILURE_FATAL="no"
IPV6INIT="yes"
IPV6_AUTOCONF="yes"
IPV6_DEFROUTE="yes"
IPV6_PEERDNS="yes"
IPV6_PEERROUTES="yes"
IPV6_FAILURE_FATAL="no"
IPV6_ADDR_GEN_MODE="stable-privacy"
NAME="ens32"
UUID="567e1cef-fb56-47c5-a8df-f960f41363f5"
DEVICE="ens32"
ONBOOT="yes"
IPADDR="192.168.184.201"#修改这里
GATEWAY="192.168.184.2"#修改这里
NETMASK="255.255.255.0"#修改这里
DNS1="192.168.184.2"#修改这里
~~~

重启网络服务

~~~shell
systemctl restart network
或者
service network restart
~~~

## 安装Hadoop环境准备

安装 epel-release

~~~shell
yum install -y epel-release
~~~

安装网络工具

~~~shell
yum install -y net-tools
~~~

关闭防火墙

~~~shell
systemctl stop firewalld
systemctl disable firewalld.service
~~~

创建用户

~~~shell
useradd bigdata
passwd bigdata
~~~

配置用户具有root权限

~~~shell
vim /etc/sudoers
~~~

~~~shell
## Allow root to run any commands anywhere
root ALL=(ALL) ALL
## Allows people in group wheel to run all commands
%wheel ALL=(ALL) ALL
#在%wheel下添加这一行
bigdata ALL=(ALL) NOPASSWD:ALL
~~~

创建存放软件包和软件的路径

~~~shell
mkdir /opt/module
mkdir /opt/software
~~~

修改目录权限

~~~shell
chown bigdata:bigdata /opt/module 
chown bigdata:bigdata /opt/software
~~~

查看

~~~shell
cd /opt/
ll
~~~

卸载自带JDK

~~~shell
rpm -qa | grep -i java | xargs -n1 rpm -e --nodeps
~~~

安装JDK

安装上传包工具

~~~shell
yum install -y lrzsz
~~~

上传jdk

解压JDK

~~~shell
tar -zxvf jdk-8u301-linux-x64.tar.gz -C /opt/module/
~~~

配置java环境变量

~~~shell
sudo vim /etc/profile.d/my_env.sh
~~~

~~~shell
#JAVA_HOME
export JAVA_HOME=/opt/module/jdk1.8.0_301
export PATH=$PATH:$JAVA_HOME/bin
~~~

刷新配置文件

~~~shell
source /etc/profile
~~~

复制虚拟机

修改IP地址

修改主机名

~~~shell
vim /etc/hostname
~~~

~~~shell
bigdata1
bigdata2
bigdata3
~~~

修改host映射

~~~shell
vim /etc/hosts
~~~

~~~shell
192.168.184.201 bigdata1
192.168.184.202 bigdata2
192.168.184.203 bigdata3
~~~

重启虚拟机

配置免密登录

root和普通用户都要来一次

~~~shell
ssh-keygen -t rsa
~~~

~~~shell
ssh-copy-id bigdata1
ssh-copy-id bigdata2
ssh-copy-id bigdata3
~~~

同步脚本

~~~shell
cd /home/bigdata
mkdir bin
cd bin
vim xsync
~~~

安装rsync

~~~shell
sudo yum install -y rsync
~~~

~~~shell
#!/bin/bash
#1. 判断参数个数
if [ $# -lt 1 ]
then
 echo Not Enough Arguement!
 exit;
fi
#2. 遍历集群所有机器
for host in bigdata1 bigdata2 bigdata3
do
 echo ==================== $host ====================
 #3. 遍历所有目录，挨个发送
 for file in $@
 do
 #4. 判断文件是否存在
 if [ -e $file ]
 then
 #5. 获取父目录
 pdir=$(cd -P $(dirname $file); pwd)
 #6. 获取当前文件的名称
 fname=$(basename $file)
 ssh $host "mkdir -p $pdir"
 rsync -av $pdir/$fname $host:$pdir
 else
 echo $file does not exists!
 fi
 done
done
~~~

~~~shell
chmod +x xsync
~~~

~~~shell
xsync /home/bigdata/bin
~~~

## 安装Hadoop

下载hadoop

https://archive.apache.org/dist/hadoop/common/hadoop-3.1.3/

https://archive.apache.org/dist/hadoop/common/hadoop-3.1.3/hadoop-3.1.3.tar.gz

下载

解压

~~~shell
 tar -zxvf hadoop-3.1.3.tar.gz -C /opt/module/
~~~

配置环境变量

~~~shell
sudo vim /etc/profile.d/my_env.sh
~~~

~~~shell
#HADOOP_HOME
export HADOOP_HOME=/opt/module/hadoop-3.1.3
export PATH=$PATH:$HADOOP_HOME/bin
export PATH=$PATH:$HADOOP_HOME/sbin
~~~

刷新配置文件

~~~shell
source /etc/profile
~~~

~~~shell
hadoop version
~~~

## Hadoop集群配置

| bigdata1         | bigdata2        | bigdata3          |
| ---------------- | --------------- | ----------------- |
| NameNode         |                 | SecondaryNameNode |
| DataNode         | DataNode        | DataNode          |
|                  | ResourceManager |                   |
| NodeManager      | NodeManager     | NodeManager       |
| JobHistoryServer |                 |                   |

配置hadoop配置文件

etc/hadoop

core-site.xml

~~~xml
<configuration>
    <!-- 指定 NameNode 的地址 -->
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://bigdata1:8020</value>
    </property>
    <!-- 指定 hadoop 数据的存储目录 -->
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/opt/module/hadoop-3.1.3/data</value>
    </property>
    <!-- 配置 HDFS 网页登录使用的静态用户为 atguigu -->
    <property>
        <name>hadoop.http.staticuser.user</name>
        <value>bigdata</value>
    </property>
</configuration>
~~~

hdfs-site.xml

~~~xml
<configuration>
   <!-- nn web 端访问地址-->
    <property>
        <name>dfs.namenode.http-address</name>
        <value>bigdata1:9870</value>
    </property>
    <!-- 2nn web 端访问地址-->
    <property>
        <name>dfs.namenode.secondary.http-address</name>
        <value>bigdata3:9868</value>
    </property>
</configuration>
~~~

yarn-site.xml

~~~xml
<configuration>
    <!-- 指定 MR 走 shuffle -->
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <!-- 指定 ResourceManager 的地址-->
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>bigdata2</value>
    </property>
    <!-- 环境变量的继承 -->
    <property>
        <name>yarn.nodemanager.env-whitelist</name>
        <value>
            JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME
        </value>
    </property>
</configuration>
~~~

mapred-site.xml

~~~xml
<configuration>
    <!-- 指定 MapReduce 程序运行在 Yarn 上 -->
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
</configuration>
~~~

配置workers

vim  /opt/module/hadoop-3.1.3/etc/hadoop/workers 

~~~conf
bigdata1
bigdata2
bigdata3
~~~

同步配置

~~~shell
xsync /opt/module/hadoop-3.1.3/etc/hadoop/
~~~

格式化hdfs

~~~shell
hdfs namenode -format
~~~

启动hdfs

~~~shell
sbin/start-dfs.sh
~~~

启动yarn

~~~shell
sbin/start-yarn.sh
~~~

测试hdfs与yarn

http://bigdata1:9870

http://bigdata2:8088

关闭hdfs

~~~shell
sbin/stop-dfs.sh
~~~

关闭yarn

~~~shell
sbin/stop-yarn.sh
~~~

要在主节点执行

运行wordcount任务报错

~~~
[2022-09-28 13:40:09.567]Container exited with a non-zero exit code 127. Error file: prelaunch.err.
Last 4096 bytes of prelaunch.err :
Last 4096 bytes of stderr :
/bin/bash: /bin/java: No such file or directory
~~~

在每一台机器上面执行

~~~shell
sudo ln -s /opt/module/jdk1.8.0_301/bin/java /bin/java
~~~

运行workcount报错

~~~
Error: Could not find or load main class org.apache.hadoop.mapreduce.v2.app.MRAppMaster
~~~

yarn-site.xml配置有问题

~~~shell
<configuration>
    <!-- 指定 MR 走 shuffle -->
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <!-- 指定 ResourceManager 的地址-->
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>bigdata2</value>
    </property>
    <!-- 环境变量的继承 -->
    <property>
        <name>yarn.nodemanager.env-whitelist</name>
        #这里配置有问题<value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
    </property>
</configuration>
~~~

配置历史服务器

配置mapred-site.xml

~~~xml
	<!-- 历史服务器端地址 -->
    <property>
        <name>mapreduce.jobhistory.address</name>
        <value>bigdata1:10020</value>
    </property>
    <!-- 历史服务器 web 端地址 -->
    <property>
        <name>mapreduce.jobhistory.webapp.address</name>
        <value>bigdata1:19888</value>
    </property>
~~~

配置日志聚集 yarn-site.xml

~~~xml
<!-- 开启日志聚集功能 -->
    <property>
        <name>yarn.log-aggregation-enable</name>
        <value>true</value>
    </property>
    <!-- 设置日志聚集服务器地址 -->
    <property>
        <name>yarn.log.server.url</name>
        <value>http://bigdata1:19888/jobhistory/logs</value>
    </property>
    <!-- 设置日志保留时间为 7 天 -->
    <property>
        <name>yarn.log-aggregation.retain-seconds</name>
        <value>604800</value>
    </property>
~~~

分发配置文件

开启历史服务器

~~~shell
mapred --daemon start historyserver
~~~

关闭历史服务器

~~~shell
mapred --daemon stop historyserver
~~~

历史服务器链接

http://bigdata1:19888/jobhistory

这个连接访问不了

测试

~~~shell
hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount /wcinput /wcoutput
~~~

脚本问题报错

~~~shell
/bin/bash^M: 坏的解释器: 没有那个文件或目录
~~~

windows与linux换行字符文体

~~~shell
sed 's/\r//' -i 文件名
~~~

## Spark Local安装

下载

https://archive.apache.org/dist/spark/

https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz

解压

~~~shell
tar -zxvf spark-3.0.0-bin-hadoop3.2.tgz -C /opt/module
~~~

~~~shell
cd /opt/module
mv spark-3.0.0-bin-hadoop3.2 spark-local
~~~

测试

启动

~~~shell
bin/spark-shell
~~~

http://bigdata1:4040

提交任务

~~~shell
bin/spark-submit \
--class org.apache.spark.examples.SparkPi \
--master local[2] \
./examples/jars/spark-examples_2.12-3.0.0.jar \
10
~~~

## Spark Standalone

| bigdata1 | bigdata2 | bigdata3 |
| -------- | -------- | -------- |
| Master   |          |          |
| Worker   | Worker   | Worker   |

~~~shell
cp spark-local spark-standalone
~~~

修改配置文件

将conf下配置文件

~~~shell
mv slaves.template slaves
~~~

~~~shell
bigdata1
bigdata2
bigdata3
~~~

~~~shell
mv spark-env.sh.template spark-env.sh
~~~

~~~shell
export JAVA_HOME=/opt/module/jdk1.8.0_301
SPARK_MASTER_HOST=bigdata1
SPARK_MASTER_PORT=7077
~~~

分发文件

~~~shell
xsync spark-standalone
~~~

启动集群

~~~shell
sbin/start-all.sh
~~~

查看进程

~~~shell
=============== bigdata1 ===============
4784 Master
4864 Worker
2818 NodeManager
2532 DataNode
2376 NameNode
4921 Jps
2989 JobHistoryServer
=============== bigdata2 ===============
2437 ResourceManager
3818 Worker
2572 NodeManager
2254 DataNode
3871 Jps
=============== bigdata3 ===============
2257 DataNode
5378 Jps
2344 SecondaryNameNode
2461 NodeManager
5325 Worker
~~~

页面

~~~shell
http://bigdata1:8080
~~~

测试

~~~shell
bin/spark-submit \
--class org.apache.spark.examples.SparkPi \
--master spark://bigdata1:7077 \
./examples/jars/spark-examples_2.12-3.0.0.jar \
10000
~~~

关闭spark集群

~~~shell
stop-all.sh 
~~~

配置历史服务器

~~~shell
mv spark-defaults.conf.template spark-defaults.conf
~~~

配置日志存储路径

~~~shell
spark.eventLog.enabled true
spark.eventLog.dir hdfs://bigdata1:8020/directory
~~~

hdfs上面创建该路径

~~~shell
hadoop fs -mkdir /directory
~~~

修改spark-env.sh,添加日志配置

~~~shell
export SPARK_HISTORY_OPTS="
-Dspark.history.ui.port=18080 
-Dspark.history.fs.logDirectory=hdfs://bigdata1:8020/directory 
-Dspark.history.retainedApplications=30"
~~~

分发配置

~~~shell
xsync conf
~~~

启动

~~~shell
sbin/start-history-server.sh
sbin/start-all.sh
~~~

提交任务

~~~shell
bin/spark-submit \
--class org.apache.spark.examples.SparkPi \
--master spark://bigdata1:7077 \
./examples/jars/spark-examples_2.12-3.0.0.jar \
10
~~~

查看历史服务器

~~~shell
http://bigdata1:18080
~~~

## Spark Yarn安装

~~~shell
cp -r  spark-local/ spark-yarn
~~~

修改hadoop配置yarn-site.xml

增加如下配置

~~~xml
    <!--是否启动一个线程检查每个任务正使用的物理内存量，如果任务超出分配值，则直接将其杀掉，默认
是 true -->
    <property>
        <name>yarn.nodemanager.pmem-check-enabled</name>
        <value>false</value>
    </property>
    <!--是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默认
    是 true -->
    <property>
        <name>yarn.nodemanager.vmem-check-enabled</name>
        <value>false</value>
    </property>
~~~

修改 conf/spark-env.sh

~~~shell
mv spark-env.sh.template spark-env.sh
~~~

~~~shell
export JAVA_HOME=/opt/module/jdk1.8.0_301
YARN_CONF_DIR=/opt/module/hadoop-3.1.3/etc/hadoop
~~~

分发配置

~~~shell
xsync module/
~~~

启动hdfs和yarn

~~~shell
myhadoop start
~~~

提交任务

~~~shell
bin/spark-submit \
--class org.apache.spark.examples.SparkPi \
--master yarn \
--deploy-mode cluster \
./examples/jars/spark-examples_2.12-3.0.0.jar \
100
~~~

打开yarn页面查看

配置历史服务器

~~~shell
mv spark-defaults.conf.template spark-defaults.conf
~~~

~~~shell
spark.eventLog.enabled true
spark.eventLog.dir hdfs://bigdata1:8020/directory
~~~

修改 spark-env.sh 文件, 添加日志配置

~~~shell
export SPARK_HISTORY_OPTS="
-Dspark.history.ui.port=18080 
-Dspark.history.fs.logDirectory=hdfs://bigdata1:8020/directory 
-Dspark.history.retainedApplications=30"
~~~

修改 spark-defaults.conf

~~~shell
spark.yarn.historyServer.address=bigdata1:18080
spark.history.ui.port=18080
~~~

同步配置

~~~shell
xsync spark-yarn/
~~~

启动历史服务器

~~~shell
sbin/start-history-server.sh
~~~

关闭历史服务器

~~~shell
sbin/stop-history-server.sh
~~~

spark脚本

~~~shell
#!/bin/bash
if [ $# -lt 1 ]
then
 echo "No Args Input..."
 exit ;
fi
case $1 in
"start")
 echo " =================== 启动 spark 历史服务器 ==================="
 echo " --------------- 启动 Spark 历史服务器 ---------------"
 ssh bigdata1 "/opt/module/spark-yarn/sbin/start-history-server.sh"

;;
"stop")
 echo " =================== 关闭 Spark 历史服务器 ==================="
 echo " --------------- 关闭 Spark 历史服务器 ---------------"
 ssh bigdata1 "/opt/module/spark-yarn/sbin/stop-history-server.sh"
;;
*)
 echo "Input Args Error..."
;;
esac
~~~

## HIVE

下载连接

[Index of /dist/hive (apache.org)](http://archive.apache.org/dist/hive/)

http://archive.apache.org/dist/hive/hive-1.2.1/apache-hive-1.2.1-bin.tar.gz

解压

~~~shell
tar -zxvf apache-hive-3.0.0-bin.tar.gz -C /opt/module/
~~~

改个名字

~~~shell
mv apache-hive-3.0.0-bin/ hive
~~~

~~~shell
mv hive-env.sh.template hive-env.sh
~~~

~~~shell
export HADOOP_HOME=/opt/module/hadoop-3.1.3
export HIVE_CONF_DIR=/opt/module/hive/conf
~~~

在hdfs上面创建hive的存储路径

~~~shell
hadoop fs -mkdir /tmp
hadoop fs -mkdir -p /user/hive/warehouse
hadoop fs -chmod g+w /tmp
hadoop fs -chmod g+w /user/hive/warehouse
~~~

vim hive-site.xml

~~~xml
<configuration>
    <property>
        <name>javax.jdo.option.ConnectionURL</name>
        <value>jdbc:mysql://192.168.184.128:3306/hive?createDatabaseIfNotExist=true</value>
        <description>JDBC connect string for a JDBC metastore</description>
    </property>

    <property>
        <name>javax.jdo.option.ConnectionDriverName</name>
        <value>com.mysql.jdbc.Driver</value>
        <description>Driver class name for a JDBC metastore</description>
    </property>

    <property>
        <name>javax.jdo.option.ConnectionUserName</name>
        <value>root</value>
        <description>username to use against metastore database</description>
    </property>

    <property>
        <name>javax.jdo.option.ConnectionPassword</name>
        <value>123456</value>
        <description>password to use against metastore database</description>
    </property>
    <property>
        <name>hive.metastore.schema.verification</name>
        <value>false</value>
    </property>
    <property>
        <name>hive.metastore.uris</name>
        <value>thrift://192.168.184.201:9083</value>
        <description>Thrift URI for the remote metastore. Used by metastore client to connect to remote metastore.
        </description>
    </property>
</configuration>
~~~

上传mysql驱动到hive lib下

~~~shell
./schematool -dbType mysql -initSchema
~~~

报错

~~~shell
Exception in thread "main" java.lang.NoSuchMethodError: com.google.common.base.Preconditions.checkArgument(ZLjava/lang/String;Ljava/lang/Object;)V
	at org.apache.hadoop.conf.Configuration.set(Configuration.java:1357)
	at org.apache.hadoop.conf.Configuration.set(Configuration.java:1338)
	at org.apache.hadoop.mapred.JobConf.setJar(JobConf.java:518)
	at org.apache.hadoop.mapred.JobConf.setJarByClass(JobConf.java:536)
	at org.apache.hadoop.mapred.JobConf.<init>(JobConf.java:430)
	at org.apache.hadoop.hive.conf.HiveConf.initialize(HiveConf.java:5112)
	at org.apache.hadoop.hive.conf.HiveConf.<init>(HiveConf.java:5075)
	at org.apache.hive.beeline.HiveSchemaTool.<init>(HiveSchemaTool.java:98)
	at org.apache.hive.beeline.HiveSchemaTool.main(HiveSchemaTool.java:1401)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:318)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:232)
~~~

删除hive中低版本的guava-14.0.1.jar包，将hadoop中的**guava-27.0-jre.jar**复制到hive的lib目录下即可。

启动

~~~shell
bin/hive
~~~

show databases;

报错

~~~shell
FAILED: HiveException java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient
~~~

没有启动元数据服务

~~~shell
hive --service metastore &
~~~

报错

~~~shell
-bash: hive: 未找到命令
~~~

没有配置环境变量

~~~shell
vim /etc/profile.d/my_env.sh
~~~

~~~shell
#HIVE
HIVE_HOME=/opt/module/hive
export PATH=$PATH:$HIVE_HOME/bin
~~~

刷新环境变量

~~~shell
source /etc/profile
~~~

启动元数据

~~~shell
hive --service metastore &
~~~

启动hive

~~~shell
hive
~~~

启动hiveserver2,通过jdbc连接

~~~shell
hive --service hiveserver2
~~~

beeline连接报错

~~~shell
Error: Could not open client transport with JDBC Uri: jdbc:hive2://bigdata1:10000: java.net.ConnectException: 拒绝连接 (Connection refused) (state=08S01,code=0)
~~~

修改hadoop core-site.xml

~~~xml
 <property>
        <name>hadoop.proxyuser.bigdata.hosts</name>
        <value>*</value>
    </property>
    <property>
        <name>hadoop.proxyuser.bigdata.groups</name>
        <value>*</value>
    </property>
~~~

hive脚本

~~~shell
#!/bin/bash
if [ $# -lt 1 ]
then
 echo "No Args Input..."
 exit ;
fi
case $1 in
"start")
 echo " =================== 启动 hive 集群 ==================="
 echo " --------------- 启动 hive ---------------"
 ssh bigdata1 "nohup /opt/module/hive/bin/hive --service metastore > /tmp/hivemetastore.log  2>&1 &"
 ssh bigdata1 "nohup /opt/module/hive/bin/hive --service hiveserver2 > /tmp/hiveserver2.log  2>&1 &"

;;
"stop")
 echo " =================== 关闭 hive 集群 ==================="
 echo " --------------- 关闭 hive ---------------"
 ssh bigdata1 "kill -9 $(netstat -nltp | grep 9083 | awk '{print $7}' | awk -F "/" '{print $1}')"
 ssh bigdata1 "kill -9 $(netstat -nltp | grep 10000 | awk '{print $7}' | awk -F "/" '{print $1}')"
;;
*)
 echo "Input Args Error..."
;;
esac

~~~

beeline脚本

~~~shell
#!/bin/bash
if [ -z "$1" ]
then
      beeline -u jdbc:hive2://bigdata1:10000 -n bigdata
fi
~~~

## HIVE高可用安装

需要借助zookeeper

修改hive-site.xml配置

~~~xml
<property>
        <name>hive.metastore.uris</name>
        <value>thrift://bigdata1:9083,thrift://bigdata2:9083,thrift://bigdata3:9083</value>
        <description>Thrift URI for the remote metastore. Used by metastore client to connect to remote metastore.
        </description>
    </property>
    <property>
        <name>hive.server2.support.dynamic.service.discovery</name>
        <value>true</value>
    </property>
    <property>
        <name>hive.server2.zookeeper.namespace</name>
        <value>hiveserver2_zk</value>
    </property>
    <property>
        <name>hive.zookeeper.quorum</name>
        <value>bigdata1:2181,bigdata2:2181,bigdata3:2181</value>
    </property>
    <property>
        <name>hive.zookeeper.client.port</name>
        <value>2181</value>
    </property>
    <property>
        <name>hive.server2.thrift.bind.host</name>
        <value>bigdata1</value>
    </property>
~~~

同步配置,修改其他hive-site.xml

~~~shell
xsync hive
~~~

~~~xml
<property>
        <name>hive.server2.thrift.bind.host</name>
        <value>bigdata2</value>
</property>
~~~

~~~xml
<property>
        <name>hive.server2.thrift.bind.host</name>
        <value>bigdata3</value>
</property>
~~~

脚本

~~~shell
#!/bin/bash
if [ $# -lt 1 ]; then
  echo "No Args Input..."
  exit
fi
case $1 in
"start")
  echo " =================== 启动 hive 集群 ==================="
  echo " --------------- 启动 bigdata1 hive ---------------"
  echo " --------------- 启动 bigdata1 metastore ---------------"
  ssh bigdata1 "nohup /opt/module/hive/bin/hive --service metastore > null  2>&1 &"
  echo " --------------- 启动 bigdata1 hiveserver2 ---------------"
  ssh bigdata1 "nohup /opt/module/hive/bin/hive --service hiveserver2 > null  2>&1 &"
  echo " --------------- 启动 bigdata2 hive ---------------"
  echo " --------------- 启动 bigdata2 metastore ---------------"
  ssh bigdata2 "nohup /opt/module/hive/bin/hive --service metastore > null  2>&1 &"
  echo " --------------- 启动 bigdata2 hiveserver2 ---------------"
  ssh bigdata2 "nohup /opt/module/hive/bin/hive --service hiveserver2 > null  2>&1 &"
  echo " --------------- 启动 bigdata3 hive ---------------"
  echo " --------------- 启动 bigdata3 metastore ---------------"
  ssh bigdata3 "nohup /opt/module/hive/bin/hive --service metastore > null  2>&1 &"
  echo " --------------- 启动 bigdata3 hiveserver2 ---------------"
  ssh bigdata3 "nohup /opt/module/hive/bin/hive --service hiveserver2 > null  2>&1 &"

  ;;
"stop")
  echo " =================== 关闭 hive 集群 ==================="
  echo " --------------- 关闭 bigdata1 hive ---------------"
  ssh bigdata1 "kill -9 $(netstat -nltp | grep 9083 | awk '{print $7}' | awk -F "/" '{print $1}')"
  ssh bigdata1 "kill -9 $(netstat -nltp | grep 10000 | awk '{print $7}' | awk -F "/" '{print $1}')"
  echo " --------------- 关闭 bigdata2 hive ---------------"
  ssh bigdata2 "kill -9 $(netstat -nltp | grep 9083 | awk '{print $7}' | awk -F "/" '{print $1}')"
  ssh bigdata2 "kill -9 $(netstat -nltp | grep 10000 | awk '{print $7}' | awk -F "/" '{print $1}')"
  echo " --------------- 关闭 bigdata3 hive ---------------"
  ssh bigdata3 "kill -9 $(netstat -nltp | grep 9083 | awk '{print $7}' | awk -F "/" '{print $1}')"
  ssh bigdata3 "kill -9 $(netstat -nltp | grep 10000 | awk '{print $7}' | awk -F "/" '{print $1}')"
  ;;
*)
  echo "Input Args Error..."
  ;;
esac
~~~

beeline脚本

~~~shell
#!/bin/bash
/opt/module/hive/bin/beeline -u "jdbc:hive2://bigdata1:2181,bigdata2:2181,bigdata3:2181/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2_zk" -n bigdata
~~~

关闭脚本报错

~~~
(Not all processes could be identified, non-owned process info
 will not be shown, you would have to be root to see it all.)
kill: 用法:kill [-s 信号声明 | -n 信号编号 | -信号声明] 进程号 | 任务声明 ... 或 kill -l [信号声明]
~~~

命令上面$前面加转义符

~~~shell
#!/bin/bash
if [ $# -lt 1 ]; then
  echo "No Args Input..."
  exit
fi
case $1 in
"start")
  echo " =================== 启动 hive 集群 ==================="
  echo " --------------- 启动 bigdata1 hive ---------------"
  echo " --------------- 启动 bigdata1 metastore ---------------"
  ssh bigdata1 "nohup /opt/module/hive/bin/hive --service metastore > null  2>&1 &"
  echo " --------------- 启动 bigdata1 hiveserver2 ---------------"
  ssh bigdata1 "nohup /opt/module/hive/bin/hive --service hiveserver2 > null  2>&1 &"
  echo " --------------- 启动 bigdata2 hive ---------------"
  echo " --------------- 启动 bigdata2 metastore ---------------"
  ssh bigdata2 "nohup /opt/module/hive/bin/hive --service metastore > null  2>&1 &"
  echo " --------------- 启动 bigdata2 hiveserver2 ---------------"
  ssh bigdata2 "nohup /opt/module/hive/bin/hive --service hiveserver2 > null  2>&1 &"
  echo " --------------- 启动 bigdata3 hive ---------------"
  echo " --------------- 启动 bigdata3 metastore ---------------"
  ssh bigdata3 "nohup /opt/module/hive/bin/hive --service metastore > null  2>&1 &"
  echo " --------------- 启动 bigdata3 hiveserver2 ---------------"
  ssh bigdata3 "nohup /opt/module/hive/bin/hive --service hiveserver2 > null  2>&1 &"

  ;;
"stop")
  echo " =================== 关闭 hive 集群 ==================="
  echo " --------------- 关闭 bigdata1 hive ---------------"
  ssh bigdata1 "kill -9 \$(netstat -nltp | grep 9083 | awk '{print \$7}' | awk -F "/" '{print \$1}')"
  ssh bigdata1 "kill -9 \$(netstat -nltp | grep 10000 | awk '{print \$7}' | awk -F "/" '{print \$1}')"
  echo " --------------- 关闭 bigdata2 hive ---------------"
  ssh bigdata2 "kill -9 \$(netstat -nltp | grep 9083 | awk '{print \$7}' | awk -F "/" '{print \$1}')"
  ssh bigdata2 "kill -9 \$(netstat -nltp | grep 10000 | awk '{print \$7}' | awk -F "/" '{print \$1}')"
  echo " --------------- 关闭 bigdata3 hive ---------------"
  ssh bigdata3 "kill -9 \$(netstat -nltp | grep 9083 | awk '{print \$7}' | awk -F "/" '{print \$1}')"
  ssh bigdata3 "kill -9 \$(netstat -nltp | grep 10000 | awk '{print \$7}' | awk -F "/" '{print \$1}')"
  ;;
*)
  echo "Input Args Error..."
  ;;
esac
~~~

## Spark SQL

将hive-site.xml复制到spark的$SPARK_HOME/conf

~~~shell
cp /opt/module/hive/conf/hive-site.xml /opt/module/spark-yarn/conf
~~~

spark-env.sh配置SPARK_CLASSPATH

~~~shell
export SPARK_CLASSPATH=$SPARK_CLASSPATH:/opt/module/hive/lib/mysql-connector-java-8.0.25.jar
~~~

本地启动

~~~shell
start-thriftserver.sh
~~~

yarn启动

~~~shell
sh ./start-thriftserver.sh --master yarn
~~~

sparkSQL启动脚本

~~~shell
#!/bin/bash
if [ $# -lt 1 ]
then
 echo "No Args Input..."
 exit ;
fi
case $1 in
"start")
 echo " =================== 启动 spark SQL集群 ==================="
 echo " --------------- 启动 Spark SQL ---------------"
 ssh bigdata1 "nohup /opt/module/hive/bin/hive --service metastore > /tmp/hivemetastore.log  2>&1 &"
 ssh bigdata1 "nohup /opt/module/spark-yarn/sbin/start-thriftserver.sh --master yarn > /tmp/hiveserver2.log  2>&1 &"

;;
"stop")
 echo " =================== 关闭 Spark SQL 集群 ==================="
 echo " --------------- 关闭 Spark SQL ---------------"
 ssh bigdata1 "kill -9 $(netstat -nltp | grep 9083 | awk '{print $7}' | awk -F "/" '{print $1}')"
 ssh bigdata1 "/opt/module/spark-yarn/sbin/stop-thriftserver.sh"
;;
*)
 echo "Input Args Error..."
;;
esac
~~~

beeline脚本

~~~shell
#!/bin/bash
if [ -z "$1" ]
then
      /opt/module/spark-yarn/bin/beeline -u jdbc:hive2://bigdata1:10000 -n bigdata
fi
~~~

Spark SQL 高可用

将hive的高可用hive-site复制到sparkconf目录下

要修改

~~~xml
<property>
        <name>hive.server2.thrift.bind.host</name>
        <value>bigdata2</value>
</property>
<property>
        <name>hive.server2.thrift.bind.host</name>
        <value>bigdata3</value>
</property>
~~~

启动脚本

~~~shell
#!/bin/bash
if [ $# -lt 1 ]; then
  echo "No Args Input..."
  exit
fi
case $1 in
"start")
  echo " =================== 启动 spark SQL集群 ==================="
  echo " --------------- 启动 bigdata1 Spark SQL ---------------"
  ssh bigdata1 "nohup /opt/module/hive/bin/hive --service metastore > null  2>&1 &"
  ssh bigdata1 "nohup /opt/module/spark-yarn/sbin/start-thriftserver.sh --master yarn > null  2>&1 &"
  echo " --------------- 启动 bigdata2 Spark SQL ---------------"
  ssh bigdata2 "nohup /opt/module/hive/bin/hive --service metastore > null  2>&1 &"
  ssh bigdata2 "nohup /opt/module/spark-yarn/sbin/start-thriftserver.sh --master yarn > null  2>&1 &"
  echo " --------------- 启动 bigdata3 Spark SQL ---------------"
  ssh bigdata3 "nohup /opt/module/hive/bin/hive --service metastore > null  2>&1 &"
  ssh bigdata3 "nohup /opt/module/spark-yarn/sbin/start-thriftserver.sh --master yarn > null  2>&1 &"
  ;;
"stop")
  echo " =================== 关闭 Spark SQL 集群 ==================="
  echo " --------------- 关闭 bigdata1 Spark SQL ---------------"
  ssh bigdata1 "kill -9 \$(netstat -nltp | grep 9083 | awk '{print \$7}' | awk -F "/" '{print \$1}')"
  ssh bigdata1 "/opt/module/spark-yarn/sbin/stop-thriftserver.sh"
  echo " --------------- 关闭 bigdata2 Spark SQL ---------------"
  ssh bigdata2 "kill -9 \$(netstat -nltp | grep 9083 | awk '{print \$7}' | awk -F "/" '{print \$1}')"
  ssh bigdata2 "/opt/module/spark-yarn/sbin/stop-thriftserver.sh"
  echo " --------------- 关闭 bigdata3 Spark SQL ---------------"
  ssh bigdata3 "kill -9 \$(netstat -nltp | grep 9083 | awk '{print \$7}' | awk -F "/" '{print \$1}')"
  ssh bigdata3 "/opt/module/spark-yarn/sbin/stop-thriftserver.sh"
  ;;
*)
  echo "Input Args Error..."
  ;;
esac
~~~

beeline脚本

~~~shell
#!/bin/bash
/opt/module/spark-yarn/bin/beeline -u "jdbc:hive2://bigdata1:2181,bigdata2:2181,bigdata3:2181/default;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2_zk" -n bigdata
~~~

## Spark SQL高可用

Spark SQL官方版本不支持注册到zookeeper上面,需要下载源码,改代码重新的打包.

日后补充

## zookeeper

解压

~~~shell
tar -zxvf apache-zookeeper-3.6.3-bin.tar.gz -C /opt/module/
~~~

集群分发

~~~shell
xsync zookeeper
~~~

配置服务器编号

~~~shell
cd zookeeper
mkdir -p zkData
cd zkData
touch myid
vi myid
1
分发我的id,并修改其他机器上面id
xsync myid
~~~

配置 zoo.cfg 文件

~~~shell
mv zoo_sample.cfg zoo.cfg
vim zoo.cfg
~~~

~~~shell
dataDir=/opt/module/zookeeper/zkData
增加如下配置
#######################cluster##########################
server.1=bigdata1:2888:3888
server.2=bigdata2:2888:3888
server.3=bigdata3:2888:3888
~~~

~~~shell
 xsync zoo.cfg
~~~

启动zookeeper

在每一台上面执行

~~~shell
bin/zkServer.sh start
~~~

关闭

~~~shell
bin/zkServer.sh stop
~~~

查看zookeeper主从关系

~~~shell
bin/zkServer.sh status
~~~

zookeeper脚本

~~~shell
#!/bin/bash
if [ $# -lt 1 ]
then
 echo "No Args Input..."
 exit ;
fi
case $1 in
"start")
 echo " =================== 启动 zookeeper 集群 ==================="
 echo " -------------- 启动 bigdata1 zookeeper ---------------"
 ssh bigdata1 "/opt/module/zookeeper/bin/zkServer.sh start"
 echo " --------------- 启动 bigdata2 zookeeper ---------------"
 ssh bigdata2 "/opt/module/zookeeper/bin/zkServer.sh start"
 echo " --------------- 启动 bigdata3 zookeeper ---------------"
 ssh bigdata3 "/opt/module/zookeeper/bin/zkServer.sh start"
;;
"stop")
 echo " =================== 关闭 zookeeper 集群 ==================="
 echo " --------------- 关闭 bigdata1 zookeeper  ---------------"
 ssh bigdata1 "/opt/module/zookeeper/bin/zkServer.sh stop"
 echo " --------------- 关闭 bigdata2 zookeeper  ---------------"
 ssh bigdata2 "/opt/module/zookeeper/bin/zkServer.sh stop"
 echo " --------------- 关闭 bigdata3 zookeeper  ---------------"
 ssh bigdata3 "/opt/module/zookeeper/bin/zkServer.sh stop"
;;
"status")
 echo " =================== zookeeper 集群状态 ==================="
 echo " --------------- bigdata1 zookeeper  ---------------"
 ssh bigdata1 "/opt/module/zookeeper/bin/zkServer.sh status"
 echo " --------------- hadoop102 bigdata2  ---------------"
 ssh bigdata2 "/opt/module/zookeeper/bin/zkServer.sh status"
 echo " --------------- bigdata3 zookeeper  ---------------"
 ssh bigdata3 "/opt/module/zookeeper/bin/zkServer.sh status"
;;
*)
 echo "Input Args Error..."
;;
esac
~~~

## kafka

解压

~~~shell
 tar -zxvf kafka_2.11-0.11.0.0.tgz -C /opt/module/
~~~

~~~shell
mv kafka_2.11-0.11.0.0/ kafka
~~~

~~~shell
cd kafka
~~~

~~~shell
mkdir logs
~~~

~~~shell
cd config/
~~~

~~~shell
vim server.properties
~~~

~~~shell
#broker 的全局唯一编号，不能重复 每一台机器不一样
broker.id=1
#删除 topic 功能使能
delete.topic.enable=true
#处理网络请求的线程数量
num.network.threads=3
#用来处理磁盘 IO 的现成数量
num.io.threads=8
#发送套接字的缓冲区大小
socket.send.buffer.bytes=102400
#接收套接字的缓冲区大小
socket.receive.buffer.bytes=102400
#请求套接字的缓冲区大小
socket.request.max.bytes=104857600 
#kafka 运行日志存放的路径
log.dirs=/opt/module/kafka/logs
#topic 在当前 broker 上的分区个数
num.partitions=1
#用来恢复和清理 data 下数据的线程数量
num.recovery.threads.per.data.dir=1
#segment 文件保留的最长时间，超时将被删除
log.retention.hours=168
#配置连接 Zookeeper 集群地址
zookeeper.connect=bigdata1:2181,bigdata2:2181,bigdata3:2181
~~~

修改的地方

~~~shell
#broker 的全局唯一编号，不能重复 每一台机器不一样
broker.id=1
#删除 topic 功能使能
delete.topic.enable=true
#kafka 运行日志存放的路径
log.dirs=/opt/module/kafka/logs
#配置连接 Zookeeper 集群地址
zookeeper.connect=bigdata1:2181,bigdata2:2181,bigdata3:2181
~~~

配置环境变量

~~~shell
sudo vim /etc/profile.d/my_env.sh
~~~

~~~shell
#KAFKA_HOME
export KAFKA_HOME=/opt/module/kafka
export PATH=$PATH:$KAFKA_HOME/bin
~~~

刷新配置文件

~~~shell
source /etc/profile
~~~

分发配置文件和kafka

~~~shell
sudo xsync /etc/profile.d
xsync kafka
~~~

刷新其他机器环境变量和修改brokerid

启动kafka

~~~shell
/opt/module/kafka/bin/kafka-server-start.sh  -daemon /opt/module/kafka/config/server.properties
~~~

关闭kafka

~~~shell
/opt/module/kafka/bin/kafka-server-stop.sh stop
~~~

脚本

~~~shell
#!/bin/bash
if [ $# -lt 1 ]
then
 echo "No Args Input..."
 exit ;
fi
case $1 in
"start")
 echo " =================== 启动 kafka 集群 ==================="
 echo " -------------- 启动 bigdata1 kafka ---------------"
 ssh bigdata1 "/opt/module/kafka/bin/kafka-server-start.sh  -daemon /opt/module/kafka/config/server.properties"
 echo " --------------- 启动 bigdata2 kafka ---------------"
 ssh bigdata2 "/opt/module/kafka/bin/kafka-server-start.sh  -daemon /opt/module/kafka/config/server.properties"
 echo " --------------- 启动 bigdata3 kafka ---------------"
 ssh bigdata3 "/opt/module/kafka/bin/kafka-server-start.sh  -daemon /opt/module/kafka/config/server.properties"
;;
"stop")
 echo " =================== 关闭 kafka 集群 ==================="
 echo " --------------- 关闭 bigdata1 kafka  ---------------"
 ssh bigdata1 "/opt/module/kafka/bin/kafka-server-stop.sh stop"
 echo " --------------- 关闭 bigdata2 kafka  ---------------"
 ssh bigdata2 "/opt/module/kafka/bin/kafka-server-stop.sh stop"
 echo " --------------- 关闭 bigdata3 kafka  ---------------"
 ssh bigdata3 "/opt/module/kafka/bin/kafka-server-stop.sh stop"
;;
*)
 echo "Input Args Error..."
;;
esac
~~~

mykafkaproducer

~~~shell
#!/bin/bash
if [ -z "$1" ]
then
      /opt/module/kafka/bin/kafka-console-producer.sh --broker-list bigdata1:9092,bigdata2:9092,bigdata3:9092 --topic test
else
     /opt/module/kafka/bin/kafka-console-producer.sh --broker-list bigdata1:9092,bigdata2:9092,bigdata3:9092 --topic $1
fi
~~~

mykafkaconsumer

~~~shell
#!/bin/bash
if [ -z "$1" ]
then
      /opt/module/kafka/bin/kafka-console-consumer.sh --bootstrap-server bigdata1:9092,bigdata2:9092,bigdata3:9092 --topic test
else
     /opt/module/kafka/bin/kafka-console-consumer.sh --bootstrap-server bigdata1:9092,bigdata2:9092,bigdata3:9092 --topic $1
fi
~~~

## flink

| bigdata1 | bigdata2 | bigdata3 |
| -------- | -------- | -------- |
|          |          | master   |
| slave    | slave    |          |

下载flink

https://archive.apache.org/dist/flink/

解压

~~~shell
tar -zxvf flink-1.10.1-bin-scala_2.12.tgz -C /opt/module
~~~

~~~shell
mv flink-1.10.1/ flink
~~~

~~~shell
cd conf
vim flink-conf.yaml
~~~

~~~shell
jobmanager.rpc.address: bigdata3
~~~

~~~shell
vim slaves
~~~

~~~shell
bigdata1
bigdata2
bigdata3
~~~

分发配置

~~~shell
xsync flink/
~~~

启动

~~~shell
start-cluster.sh 
~~~

关闭

~~~shell
stop-cluster.sh
~~~

页面

http://bigdata3:8081/#/overview

安装nc

~~~shell
yum install -y nc 
~~~

提交任务

~~~shell
flink run  -c cn.ityege.Flink finkconsumer-1.0.jar
~~~

flinkweb端口号 8081

## flink on yarn

 Session Cluster

启动hadoop集群

配置环境变量

~~~shell
sudo vim /etc/profile.d/my_env.sh
~~~

~~~shell
#Flink
export HADOOP_CONF_DIR=/opt/module/hadoop-3.1.3/etc/hadoop
export HADOOP_CLASSPATH=`hadoop classpath`
~~~

分发配置

~~~shell
sudo /home/bigdata/xsync /etc/profile.d
~~~

刷新配置

~~~shell
source /etc/profile
~~~

 启动 yarn-session

~~~shell
./yarn-session.sh -n 2 -s 2 -jm 1024 -tm 1024 -nm test -d
~~~

~~~shell
nohup /opt/module/flink/bin/yarn-session.sh -n 2 -s 2 -jm 1024 -tm 1024 -nm test -d >/dev/null 2>&1 &
~~~

关闭集群

~~~shell
yarn application --kill application_1577588252906_0001
~~~

提交任务

~~~shell
/opt/module/flink/bin/flink run -yid application_1668871840328_0009  -c cn.ityege.Flink  finkconsumer-1.0.jar
~~~

Per Job Cluster

~~~shell
/opt/module/flink/bin/flink run -m yarn-cluster  -c cn.ityege.Flink finkconsumer-1.0.jar
~~~

脚本

~~~shell
#!/bin/bash
ssh bigdata3  "nohup /opt/module/flink/bin/yarn-session.sh -n 2 -s 2 -jm 1024 -tm 1024 -nm test -d >/dev/null 2>&1 &"
~~~

## Hbase

下载

官网下载

解压安装

配置配置文件

hbase-env.sh 添加

~~~shell
export JAVA_HOME=/opt/module/jdk1.8.0_301
export HBASE_MANAGES_ZK=false
export HBASE_CLASSPATH=/opt/module/hadoop-3.1.3/etc/hadoop
~~~

修改hbase-site.xml 

~~~xml
<configuration>
    <!-- 指定hbase在HDFS上存储的路径 -->
    <property>
        <name>hbase.rootdir</name>
        <value>hdfs://bigdata1:8020/hbase</value>
    </property>
    <!-- 指定hbase是否分布式运行 -->
    <property>
        <name>hbase.cluster.distributed</name>
        <value>true</value>
    </property>
    <!-- 0.98后的新变动，之前版本没有.port,默认端口为60000 -->
    <property>
        <name>hbase.master.port</name>
        <value>16000</value>
    </property>
    <!-- 指定zookeeper的地址，多个用“,”分割 -->
    <property>
        <name>hbase.zookeeper.quorum</name>
        <value>bigdata1:2181,bigdata2:2181,bigdata3:2181
        </value>
    </property>
    <!-- 在分布式的情况下一定要设置，不然容易出现Hmaster起不来的情况 -->
    <property>
        <name>hbase.unsafe.stream.capability.enforce</name>
        <value>false</value>
    </property>
    <property>
        <name>hbase.tmp.dir</name>
        <value>./tmp</value>
    </property>
</configuration>
~~~

修改regionservers文件

~~~s
bigdata1
bigdata2
bigdata3
~~~

修改backup-masters文件

​        创建back-masters配置文件，里边包含备份HMaster节点的主机名，每个机器独占一行，实现HMaster的高可用（原文件不存在，需要新建）

vim backup-masters

~~~
bigdata1
bigdata3
~~~

分发Hbase

启动hbase

启动之前启动zookeeper

~~~shell
/opt/module/hbase-2.4.15/bin/start-hbase.sh
~~~

关闭hbase

~~~shell
/opt/module/hbase-2.4.15/bin/hbase-daemons.sh stop master
/opt/module/hbase-2.4.15/bin/hbase-daemons.sh stop regionserver
#下面这个命令关不上,用上面那个一个一个关
/opt/module/hbase-2.4.15/bin/stop-hbase.sh
~~~

web管理页面

http://bigdata2:16010/master-status

脚本

~~~shell
#!/bin/bash
if [ $# -lt 1 ]; then
  echo "No Args Input..."
  exit
fi
case $1 in
"start")
  echo " =================== 启动 Hbase 集群 ==================="
  ssh bigdata2 "nohup /opt/module/hbase-2.4.15/bin/start-hbase.sh >> /dev/null  2>&1 &"
  ;;
"stop")
  echo " =================== 关闭 Hbase 集群 ==================="
  ssh bigdata2 "nohup /opt/module/hbase-2.4.15/bin/hbase-daemons.sh stop master >> /dev/null  2>&1 &"
  ssh bigdata2 "nohup /opt/module/hbase-2.4.15/bin/hbase-daemons.sh stop regionserver >> /dev/null  2>&1 &"
  ;;
*)
  echo "Input Args Error..."
  ;;
esac
~~~

## Phoenix

解压

复制phoenix-server-hbase-2.4-5.1.2.jar到hbase的lib目录下

~~~shell
cp /opt/module/phoenix-hbase-2.4-5.1.2-bin/phoenix-server-hbase-2.4-5.1.2.jar /opt/module/hbase-2.4.15/lib
~~~

分发phoenix-server-hbase-2.4-5.1.2.jar 

~~~shell
xsync /opt/module/hbase-2.4.15/lib/phoenix-server-hbase-2.4-5.1.2.jar
~~~

启动hbase

连接

~~~shell
sqlline.py bigdata1,bigdata2,bigdata3:2181
~~~

~~~
使用Dbeaver工具进行连接
~~~

## Storm

storm依赖zookeeper

要先启动zookeeper

Storm官网好像要安装JDK和Python,虚拟机已经安装了anaconda,就不安装python.

解压 Storm

配置

| 主机     | 管理   | DRPC | web界面 | 工作       |
| -------- | ------ | ---- | ------- | ---------- |
| bigdata1 |        |      |         | Supervisor |
| bigdata2 | Nimbus |      | ui      | Supervisor |
| bigdata3 |        | DPRC |         | Supervisor |

配置storm.yaml

配置zookeeper

~~~yaml
storm.zookeeper.servers:
     - "bigdata1"
     - "bigdata2"
     - "bigdata3
~~~

配置storm的临时目录

~~~yaml
storm.local.dir: "/opt/module/apache-storm-2.4.0/status"
~~~

配置管理节点nimbus

~~~yaml
nimbus.seeds: ["bigdata2"]
~~~

配置supervisor

~~~shell
工作节点不需要配置
~~~

配置supervisor的端口号

~~~yaml
supervisor.slots.ports:
    - 6700
    - 6701
    - 6702
    - 6703
~~~

配置storm ui的端口号

默认的端口号是8080,8080端口号被zookeeper占用,zookeeper是公用的,尽量不修改.

随便修改一个,尽量不要和spark hbase这些应用的端口号重了.

~~~yaml
ui.port: 7070
~~~

三台机器同步配置

~~~shell
xsync apache-storm-2.4.0/
~~~

配置环境变量

~~~shell
sudo vim /etc/profile.d/my_env.sh
~~~

~~~shell
#STORM
export STORM_HOME=/opt/module/apache-storm-2.4.0
export PATH=$STORM_HOME/bin:$PATH
~~~

同步环境变量

~~~shell
sudo /home/bigdata/bin/xsync /etc/profile.d/my_env.sh
~~~

启动

启动zookeeper

启动storm

~~~shell
nohup storm nimbus >/dev/null 2>&1 &
nohup storm ui >/dev/null 2>&1 &
nohup storm supervisor >/dev/null 2>&1 &
~~~

访问页面

http://bigdata2:7070/

上传topology

~~~shell
storm jar storm-1.0-SNAPSHOT.jar cn.ityege.storm.storm.ExclamationTopology stormTest
~~~

停止topology

~~~shell
storm kill stormTest
~~~

![1673786836026](assets/1673786836026.png)

Supervisor会启动LogWriter和Worker进程

一键启动脚本

~~~shell
#!/bin/bash
if [ $# -lt 1 ]; then
  echo "No Args Input..."
  exit
fi
case $1 in
"start")
  echo " =================== 启动 storm 集群 ==================="
  echo " -------------- 启动 bigdata1 nimbus ---------------"
  ssh bigdata1 "nohup /opt/module/apache-storm-2.4.0/bin/storm nimbus >/dev/null 2>&1 &"
  echo " -------------- 启动 bigdata2 nimbus ---------------"
  ssh bigdata2 "nohup /opt/module/apache-storm-2.4.0/bin/storm nimbus >/dev/null 2>&1 &"
  echo " -------------- 启动 bigdata3 nimbus ---------------"
  ssh bigdata3 "nohup /opt/module/apache-storm-2.4.0/bin/storm nimbus >/dev/null 2>&1 &"
  echo " --------------- 启动 bigdata2 ui ---------------"
  ssh bigdata2 "nohup /opt/module/apache-storm-2.4.0/bin/storm ui >/dev/null 2>&1 &"
  echo " --------------- 启动 bigdata1 supervisor ---------------"
  ssh bigdata1 "nohup /opt/module/apache-storm-2.4.0/bin/storm supervisor >/dev/null 2>&1 &"
  echo " --------------- 启动 bigdata2 supervisor ---------------"
  ssh bigdata2 "nohup /opt/module/apache-storm-2.4.0/bin/storm supervisor >/dev/null 2>&1 &"
  echo " --------------- 启动 bigdata3 supervisor ---------------"
  ssh bigdata3 "nohup /opt/module/apache-storm-2.4.0/bin/storm supervisor >/dev/null 2>&1 &"
  ;;
"stop")
  echo " =================== 关闭 storm 集群 ==================="
  echo " -------------- 关闭 bigdata1 nimbus ---------------"
  ssh bigdata1 "kill -9 \$(jps|grep Nimbus|awk '{print \$1}')"
  echo " -------------- 关闭 bigdata2 nimbus ---------------"
  ssh bigdata2 "kill -9 \$(jps|grep Nimbus|awk '{print \$1}')"
  echo " -------------- 关闭 bigdata3 nimbus ---------------"
  ssh bigdata3 "kill -9 \$(jps|grep Nimbus|awk '{print \$1}')"
  echo " --------------- 关闭 bigdata2 ui ---------------"
  ssh bigdata2 "kill -9 \$(jps|grep UIServer |awk '{print \$1}')"
  echo " --------------- 关闭 bigdata1 supervisor ---------------"
  ssh bigdata1 "kill -9 \$(jps|grep Supervisor|awk '{print \$1}')"
  echo " --------------- 关闭 bigdata2 supervisor ---------------"
  ssh bigdata2 "kill -9 \$(jps|grep Supervisor|awk '{print \$1}')"
  echo " --------------- 关闭 bigdata3 supervisor ---------------"
  ssh bigdata3 "kill -9 \$(jps|grep Supervisor|awk '{print \$1}')"
  ;;
*)
  echo "Input Args Error..."
  ;;
esac
~~~

## Storm配置DRPC

配置文件apache-storm-2.4.0/conf/storm.yaml

~~~yaml
## Locations of the drpc servers
drpc.servers:
    - "bigdata3"
drpc.childopts: "-Xmx1024m"
~~~

修改启动脚本

~~~shell
#!/bin/bash
if [ $# -lt 1 ]; then
  echo "No Args Input..."
  exit
fi
case $1 in
"start")
  echo " =================== 启动 storm 集群 ==================="
  echo " -------------- 启动 bigdata1 nimbus ---------------"
  ssh bigdata1 "nohup /opt/module/apache-storm-2.4.0/bin/storm nimbus >/dev/null 2>&1 &"
  echo " -------------- 启动 bigdata2 nimbus ---------------"
  ssh bigdata2 "nohup /opt/module/apache-storm-2.4.0/bin/storm nimbus >/dev/null 2>&1 &"
  echo " -------------- 启动 bigdata3 nimbus ---------------"
  ssh bigdata3 "nohup /opt/module/apache-storm-2.4.0/bin/storm nimbus >/dev/null 2>&1 &"
  echo " --------------- 启动 bigdata2 ui ---------------"
  ssh bigdata2 "nohup /opt/module/apache-storm-2.4.0/bin/storm ui >/dev/null 2>&1 &"
  echo " --------------- 启动 bigdata1 supervisor ---------------"
  ssh bigdata1 "nohup /opt/module/apache-storm-2.4.0/bin/storm supervisor >/dev/null 2>&1 &"
  echo " --------------- 启动 bigdata2 supervisor ---------------"
  ssh bigdata2 "nohup /opt/module/apache-storm-2.4.0/bin/storm supervisor >/dev/null 2>&1 &"
  echo " --------------- 启动 bigdata3 supervisor ---------------"
  ssh bigdata3 "nohup /opt/module/apache-storm-2.4.0/bin/storm supervisor >/dev/null 2>&1 &"
  echo " --------------- 启动 bigdata3 drpc ---------------"
  ssh bigdata3 "nohup /opt/module/apache-storm-2.4.0/bin/storm drpc >/dev/null 2>&1 &"
  ;;
"stop")
  echo " =================== 关闭 storm 集群 ==================="
  echo " -------------- 关闭 bigdata1 nimbus ---------------"
  ssh bigdata1 "kill -9 \$(jps|grep Nimbus|awk '{print \$1}')"
  echo " -------------- 关闭 bigdata2 nimbus ---------------"
  ssh bigdata2 "kill -9 \$(jps|grep Nimbus|awk '{print \$1}')"
  echo " -------------- 关闭 bigdata3 nimbus ---------------"
  ssh bigdata3 "kill -9 \$(jps|grep Nimbus|awk '{print \$1}')"
  echo " --------------- 关闭 bigdata2 ui ---------------"
  ssh bigdata2 "kill -9 \$(jps|grep UIServer |awk '{print \$1}')"
  echo " --------------- 关闭 bigdata1 supervisor ---------------"
  ssh bigdata1 "kill -9 \$(jps|grep Supervisor|awk '{print \$1}')"
  echo " --------------- 关闭 bigdata2 supervisor ---------------"
  ssh bigdata2 "kill -9 \$(jps|grep Supervisor|awk '{print \$1}')"
  echo " --------------- 关闭 bigdata3 supervisor ---------------"
  ssh bigdata3 "kill -9 \$(jps|grep Supervisor|awk '{print \$1}')"
  echo " --------------- 关闭 bigdata3 drpc ---------------"
  ssh bigdata3 "kill -9 \$(jps|grep DRPCServer|awk '{print \$1}')"
  ;;
*)
  echo "Input Args Error..."
  ;;
esac
~~~

